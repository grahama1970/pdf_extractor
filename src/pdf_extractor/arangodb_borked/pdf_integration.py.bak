#!/usr/bin/env python3
"""
PDF Extractor ArangoDB Integration Module

This module provides functions for integrating the PDF extractor with ArangoDB,
enabling efficient storage and retrieval of extracted PDF content using various
query methods.
"""

import os
import sys
import logging
import json
from datetime import datetime
from typing import Dict, Any, List, Optional, Tuple, Union

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Import ArangoDB connection module
try:
    from pdf_extractor.arangodb.connection import get_db, create_collections
except ImportError as e:
    logger.error(f"Failed to import connection module: {e}")
    logger.error("Please ensure the ArangoDB connection module is available")

# PDF collection name - can be overridden with environment variable
PDF_COLLECTION_NAME = os.getenv('PDF_COLLECTION_NAME', 'pdf_documents')

def setup_pdf_collection(db):
    """
    Set up the collection for storing PDF extraction results.
    
    Args:
        db: ArangoDB database connection
        
    Returns:
        Collection object if successful, None otherwise
    """
    try:
        # Create or get collections
        collections = create_collections(db, [PDF_COLLECTION_NAME])
        
        if PDF_COLLECTION_NAME not in collections:
            logger.error(f"Failed to create or access collection: {PDF_COLLECTION_NAME}")
            return None
            
        collection = collections[PDF_COLLECTION_NAME]
        
        # Create necessary indexes if they don't exist
        indexes = collection.indexes()
        
        # Check if indexes already exist by type and fields
        has_type_index = False
        has_file_path_index = False
        has_page_index = False
        has_fulltext_index = False
        
        for idx in indexes:
            idx_type = idx.get('type')
            idx_fields = idx.get('fields', [])
            
            if idx_type == 'hash' and 'type' in idx_fields:
                has_type_index = True
                
            if idx_type == 'hash' and 'file_path' in idx_fields:
                has_file_path_index = True
                
            if idx_type == 'skiplist' and 'page' in idx_fields:
                has_page_index = True
                
            if idx_type == 'fulltext' and 'text' in idx_fields:
                has_fulltext_index = True
        
        # Create indexes if needed
        if not has_type_index:
            collection.add_index({
                'type': 'hash',
                'fields': ['type'],
                'unique': False
            })
            logger.info("Created hash index on 'type' field")
            
        if not has_file_path_index:
            collection.add_index({
                'type': 'hash',
                'fields': ['file_path'],
                'unique': False
            })
            logger.info("Created hash index on 'file_path' field")
            
        if not has_page_index:
            collection.add_index({
                'type': 'skiplist',
                'fields': ['page'],
                'unique': False
            })
            logger.info("Created skiplist index on 'page' field")
            
        if not has_fulltext_index:
            collection.add_index({
                'type': 'fulltext',
                'fields': ['text'],
                'minLength': 3
            })
            logger.info("Created fulltext index on 'text' field")
            
        return collection
    except Exception as e:
        logger.error(f"Failed to set up PDF collection: {e}")
        return None

def store_pdf_content(collection, content_items):
    """
    Store PDF extraction results in ArangoDB.
    
    Args:
        collection: ArangoDB collection
        content_items: List of extracted content items or a single item
        
    Returns:
        Number of successfully stored items
    """
    # Convert single item to list if needed
    if not isinstance(content_items, list):
        content_items = [content_items]
        
    success_count = 0
    validation_failures = {}
    
    for i, item in enumerate(content_items):
        # Validate required fields
        required_fields = ["type", "file_path", "extraction_date"]
        
        for field in required_fields:
            if field not in item:
                validation_failures[f"item_{i}_{field}"] = {
                    "expected": f"Field '{field}' to be present",
                    "actual": "Field missing"
                }
                
        # Type-specific validation
        if item.get("type") == "heading" and "level" not in item:
            validation_failures[f"item_{i}_heading_level"] = {
                "expected": "Field 'level' for heading",
                "actual": "Field missing"
            }
            
        if item.get("type") == "table":
            if "headers" not in item:
                validation_failures[f"item_{i}_table_headers"] = {
                    "expected": "Field 'headers' for table",
                    "actual": "Field missing"
                }
                
            if "rows" not in item:
                validation_failures[f"item_{i}_table_rows"] = {
                    "expected": "Field 'rows' for table",
                    "actual": "Field missing"
                }
                
        # Skip insertion if validation failed
        if validation_failures:
            continue
            
        try:
            # Insert the document
            collection.insert(item)
            success_count += 1
        except Exception as e:
            logger.error(f"Failed to insert item {i}: {e}")
            validation_failures[f"item_{i}_insertion"] = {
                "expected": "Successful insertion",
                "actual": f"Error: {str(e)}"
            }
    
    # Report validation failures
    if validation_failures:
        logger.error("Validation failures during document insertion:")
        for field, details in validation_failures.items():
            logger.error(f"  - {field}: Expected: {details['expected']}, Got: {details['actual']}")
    
    return success_count

def query_pdf_content(db, collection_name=PDF_COLLECTION_NAME, **kwargs):
    """
    Query PDF content with various filters.
    
    Args:
        db: ArangoDB database connection
        collection_name: Name of the collection to query
        **kwargs: Query parameters:
            - search_text: Text to search for
            - doc_type: Document type (heading, paragraph, table)
            - file_path: File path to filter by
            - page: Page number to filter by
            - limit: Maximum number of results (default: 10)
            - offset: Number of results to skip (default: 0)
            
    Returns:
        List of matching documents
    """
    # Extract query parameters
    search_text = kwargs.get('search_text')
    doc_type = kwargs.get('doc_type')
    file_path = kwargs.get('file_path')
    page = kwargs.get('page')
    limit = kwargs.get('limit', 10)
    offset = kwargs.get('offset', 0)
    
    try:
        # Build query
        filter_conditions = []
        bind_vars = {
            "limit": limit,
            "offset": offset
        }
        
        # Add filter conditions
        if doc_type:
            filter_conditions.append("doc.type == @doc_type")
            bind_vars["doc_type"] = doc_type
            
        if file_path:
            filter_conditions.append("doc.file_path == @file_path")
            bind_vars["file_path"] = file_path
            
        if page is not None:
            filter_conditions.append("doc.page == @page")
            bind_vars["page"] = page
            
        # Determine query type
        if search_text:
            # Fulltext search
            aql = f"""
            FOR doc IN FULLTEXT({collection_name}, "text", @search_text)
            """
            bind_vars["search_text"] = search_text
        else:
            # Regular collection scan
            aql = f"""
            FOR doc IN {collection_name}
            """
            
        # Add filter conditions
        if filter_conditions:
            aql += f" FILTER {' AND '.join(filter_conditions)}"
            
        # Add sorting and pagination
        aql += """
            SORT doc.page ASC, doc.token_count DESC
            LIMIT @offset, @limit
            RETURN doc
        """
        
        # Execute query
        cursor = db.aql.execute(aql, bind_vars=bind_vars)
        results = [doc for doc in cursor]
        
        return results
    except Exception as e:
        logger.error(f"Query failed: {e}")
        return []

def get_pdf_content_stats(db, collection_name=PDF_COLLECTION_NAME):
    """
    Get statistics about the stored PDF content.
    
    Args:
        db: ArangoDB database connection
        collection_name: Name of the collection
        
    Returns:
        Dictionary with statistics
    """
    try:
        stats = {
            "total_documents": 0,
            "type_counts": {},
            "file_counts": {},
            "page_distribution": {}
        }
        
        # Total count
        aql = f"""
        RETURN LENGTH({collection_name})
        """
        cursor = db.aql.execute(aql)
        stats["total_documents"] = next(cursor)
        
        # Type counts
        aql = f"""
        FOR doc IN {collection_name}
            COLLECT type = doc.type WITH COUNT INTO count
            RETURN {{
                type: type,
                count: count
            }}
        """
        cursor = db.aql.execute(aql)
        type_stats = [item for item in cursor]
        for item in type_stats:
            stats["type_counts"][item["type"]] = item["count"]
            
        # File counts
        aql = f"""
        FOR doc IN {collection_name}
            COLLECT file = doc.file_path WITH COUNT INTO count
            RETURN {{
                file: file,
                count: count
            }}
        """
        cursor = db.aql.execute(aql)
        file_stats = [item for item in cursor]
        for item in file_stats:
            stats["file_counts"][item["file"]] = item["count"]
            
        # Page distribution
        aql = f"""
        FOR doc IN {collection_name}
            FILTER doc.page != null
            COLLECT page = doc.page WITH COUNT INTO count
            SORT page ASC
            RETURN {{
                page: page,
                count: count
            }}
        """
        cursor = db.aql.execute(aql)
        page_stats = [item for item in cursor]
        for item in page_stats:
            stats["page_distribution"][item["page"]] = item["count"]
            
        return stats
    except Exception as e:
        logger.error(f"Failed to get PDF content stats: {e}")
        return {"error": str(e)}

def find_headings_with_content(db, collection_name=PDF_COLLECTION_NAME, file_path=None, limit=10):
    """
    Find headings and their associated content.
    
    Args:
        db: ArangoDB database connection
        collection_name: Name of the collection
        file_path: Optional file path to filter by
        limit: Maximum number of headings to return
        
    Returns:
        List of headings with their associated content
    """
    try:
        # Prepare bind variables
        bind_vars = {"limit": limit}
        file_filter = ""
        
        if file_path:
            bind_vars["file_path"] = file_path
            file_filter = "AND doc.file_path == @file_path"
            
        # Find headings
        aql = f"""
        LET headings = (
            FOR doc IN {collection_name}
                FILTER doc.type == "heading" {file_filter}
                SORT doc.page ASC, doc.level ASC
                LIMIT @limit
                RETURN doc
        )
        
        FOR heading IN headings
            LET content = (
                FOR doc IN {collection_name}
                    FILTER doc.type != "heading" AND 
                          doc.file_path == heading.file_path AND
                          doc.page >= heading.page
                    SORT doc.page ASC, doc.token_count DESC
                    LIMIT 10
                    RETURN doc
            )
            
            RETURN {{
                heading: heading,
                content: content
            }}
        """
        
        cursor = db.aql.execute(aql, bind_vars=bind_vars)
        results = [item for item in cursor]
        
        return results
    except Exception as e:
        logger.error(f"Failed to find headings with content: {e}")
        return []

def validate_query_results(expected_keys, actual_results, query_name):
    """
    Validate query results against expected keys.
    
    Args:
        expected_keys: List of expected document keys
        actual_results: Actual query results
        query_name: Name of the query for reporting
        
    Returns:
        Tuple of (validation_passed, validation_failures)
    """
    validation_failures = {}
    
    # Check if we have any results
    if not actual_results:
        if expected_keys:
            validation_failures["no_results"] = {
                "expected": f"{len(expected_keys)} results",
                "actual": "No results"
            }
            return False, validation_failures
        else:
            # If we expect no results and got none, that's valid
            return True, {}
    
    # Check if we have the expected number of results
    actual_keys = [doc.get("_key") for doc in actual_results]
    
    # Find missing keys
    missing_keys = set(expected_keys) - set(actual_keys)
    if missing_keys:
        validation_failures["missing_keys"] = {
            "expected": f"Keys {list(missing_keys)} to be present",
            "actual": "Keys not found in results"
        }
    
    # Report validation status
    validation_passed = len(validation_failures) == 0
    
    if not validation_passed:
        logger.error(f"❌ {query_name} validation failed:")
        for field, details in validation_failures.items():
            logger.error(f"  - {field}: Expected: {details['expected']}, Got: {details['actual']}")
    else:
        logger.info(f"✅ {query_name} validation passed")
    
    return validation_passed, validation_failures

if __name__ == "__main__":
    """
    Main execution for testing and validation.
    """
    logger.info("=== PDF Extractor ArangoDB Integration ===")
    
    # Connect to ArangoDB
    try:
        db = get_db()
        if not db:
            logger.error("Failed to connect to ArangoDB")
            sys.exit(1)
            
        logger.info(f"Connected to ArangoDB database: {db.name}")
    except Exception as e:
        logger.error(f"Failed to connect to ArangoDB: {e}")
        sys.exit(1)
    
    # Set up PDF collection
    collection = setup_pdf_collection(db)
    if not collection:
        logger.error("Failed to set up PDF collection")
        sys.exit(1)
        
    logger.info(f"Using collection: {collection.name}")
    
    # Generate test data
    test_id = datetime.now().strftime("%Y%m%d%H%M%S")
    test_data = [
        {
            "_key": f"test_heading_{test_id}",
            "type": "heading",
            "level": 1,
            "text": "Test Heading",
            "page": 1,
            "token_count": 2,
            "file_path": "test.pdf",
            "extraction_date": datetime.now().isoformat(),
            "source": "test"
        },
        {
            "_key": f"test_paragraph_{test_id}",
            "type": "paragraph",
            "text": "This is a test paragraph for validation.",
            "page": 1,
            "token_count": 7,
            "file_path": "test.pdf",
            "extraction_date": datetime.now().isoformat(),
            "source": "test"
        }
    ]
    
    # Store test data
    stored_count = store_pdf_content(collection, test_data)
    logger.info(f"Stored {stored_count} test documents")
    
    # Check if data was stored
    if stored_count != len(test_data):
        logger.error("❌ Data storage validation failed")
        sys.exit(1)
    
    # Validate retrieval by type
    type_results = query_pdf_content(db, doc_type="heading")
    type_validation, type_failures = validate_query_results(
        [test_data[0]["_key"]], 
        type_results, 
        "Type query"
    )
    
    # Validate retrieval by text
    text_results = query_pdf_content(db, search_text="test paragraph")
    text_validation, text_failures = validate_query_results(
        [test_data[1]["_key"]], 
        text_results, 
        "Text search"
    )
    
    # Validate combined query
    combined_results = query_pdf_content(
        db, 
        doc_type="paragraph", 
        file_path="test.pdf"
    )
    combined_validation, combined_failures = validate_query_results(
        [test_data[1]["_key"]], 
        combined_results, 
        "Combined query"
    )
    
    # Get statistics
    stats = get_pdf_content_stats(db)
    logger.info("PDF content statistics:")
    logger.info(f"  Total documents: {stats.get('total_documents', 0)}")
    logger.info(f"  Type counts: {stats.get('type_counts', {})}")
    
    # Clean up test data
    for item in test_data:
        try:
            collection.delete(item["_key"])
        except Exception as e:
            logger.warning(f"Failed to delete test document {item['_key']}: {e}")
    
    # Final validation report
    all_passed = type_validation and text_validation and combined_validation
    
    if all_passed:
        logger.info("✅ All validations passed")
        sys.exit(0)
    else:
        logger.error("❌ One or more validations failed")
        sys.exit(1)
